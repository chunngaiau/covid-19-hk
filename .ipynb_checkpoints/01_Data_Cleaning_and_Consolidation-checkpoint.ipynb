{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Consolidation\n",
    "\n",
    "Currently, the data are separated in daily csv files that could have different formats. Our goal in this notebook is to consolidate the daily csvs into a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 148 CSV Files\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = os.path.join('data', 'enhanced_sur')\n",
    "csv_filenames = {}\n",
    "\n",
    "for file in os.listdir(CSV_PATH):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_filenames[file[:8]] = file\n",
    "\n",
    "print('Found {} CSV Files'.format(len(csv_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = []\n",
    "\n",
    "for key in csv_filenames:\n",
    "    csvs.append(pd.read_csv(os.path.join(CSV_PATH, csv_filenames[key])))\n",
    "    csvs[-1].month = key[4:6]\n",
    "    csvs[-1].day = key[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "See if the columns have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 04 Index(['Case no.', 'Report date', 'Date of onset ', 'Gender', 'Age',\n",
      "       'Name of hospital admitted', 'Hospitalised/Discharged/Deceased',\n",
      "       'HK/Non-HK resident', 'Case classification*', 'Confirmed/probable'],\n",
      "      dtype='object')\n",
      "04 07 Index(['Date of onset'], dtype='object')\n",
      "04 14 Index(['Date of onset '], dtype='object')\n",
      "04 21 Index(['Date of onset'], dtype='object')\n",
      "04 28 Index(['Date of onset '], dtype='object')\n",
      "05 05 Index(['Date of onset'], dtype='object')\n",
      "05 12 Index(['Date of onset '], dtype='object')\n",
      "05 19 Index(['Date of onset'], dtype='object')\n",
      "05 27 Index(['Date of onset '], dtype='object')\n",
      "06 02 Index(['Date of onset'], dtype='object')\n",
      "06 09 Index(['Date of onset '], dtype='object')\n",
      "06 16 Index(['Date of onset'], dtype='object')\n",
      "06 23 Index(['Date of onset '], dtype='object')\n",
      "06 30 Index(['Date of onset'], dtype='object')\n",
      "07 07 Index(['Date of onset '], dtype='object')\n",
      "07 14 Index(['Date of onset'], dtype='object')\n",
      "07 21 Index(['Date of onset '], dtype='object')\n",
      "07 28 Index(['Date of onset'], dtype='object')\n",
      "08 05 Index(['Date of onset '], dtype='object')\n",
      "08 06 Index(['Date of onset'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "last_columns = None\n",
    "\n",
    "for csv in csvs:\n",
    "    if not all(last_columns == csv.columns):\n",
    "        print(csv.month, csv.day, csv.columns[csv.columns != last_columns])\n",
    "        last_columns = csv.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the column for date of onset alternates between having a trailing space and not each week. So, all we need to change is the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csvs:\n",
    "    try:\n",
    "        csv.rename(columns=(lambda col: col.strip()), inplace=True)\n",
    "    except:\n",
    "        print(csv.month, csv.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csvs:\n",
    "    try:\n",
    "        csv['Report date'] = pd.to_datetime(csv['Report date'], dayfirst=True)\n",
    "    except:\n",
    "        print(csv.month, csv.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csvs:\n",
    "    csv['Asymptomatic'] = csv['Date of onset'] == 'Asymptomatic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_map(date):\n",
    "    date = date.title()\n",
    "    if 'Jan' in date:\n",
    "        date = '01/01/2020'\n",
    "    elif 'Feb' in date:\n",
    "        date = '01/02/2020'\n",
    "    elif 'Mar' in date:\n",
    "        date = '01/03/2020'\n",
    "    elif 'Apr' in date:\n",
    "        date = '01/04/2020'\n",
    "    elif 'May' in date:\n",
    "        date = '01/05/2020'\n",
    "    elif 'Jun' in date:\n",
    "        date = '01/06/2020'\n",
    "    elif 'Jul' in date:\n",
    "        date = '01/07/2020'\n",
    "    elif 'Aug' in date:\n",
    "        date = '01/08/2020'\n",
    "    elif 'Sep' in date:\n",
    "        date = '01/09/2020'\n",
    "    elif 'Oct' in date:\n",
    "        date = '01/10/2020'\n",
    "    elif 'Nov' in date:\n",
    "        date = '01/11/2020'\n",
    "    elif 'Dec' in date:\n",
    "        date = '01/12/2020'\n",
    "    return date\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Date of onset'] = csv['Date of onset'].map(dates_map)\n",
    "    csv['Date of onset'] = pd.to_datetime(csv['Date of onset'], errors='coerce', dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M          147487\n",
       "F          138882\n",
       "Pending         1\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genders = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Gender'] = csv['Gender'].map(lambda ele: ele.title())\n",
    "    if all_genders is None:\n",
    "        all_genders = csv['Gender']\n",
    "    else:\n",
    "        all_genders = all_genders.append(csv['Gender'], ignore_index=True)\n",
    "\n",
    "all_genders.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_map(gender):\n",
    "    if gender == 'Pending':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return gender\n",
    "\n",
    "csv['Gender'] = csv['Gender'].map(gender_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of Hospital Admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nan                                         167450\n",
       "Princess Margaret Hospital                   16194\n",
       "Queen Elizabeth Hospital                     13658\n",
       "Pamela Youde Nethersole Eastern Hospital     13639\n",
       "United Christian Hospital                    13611\n",
       "Queen Mary Hospital                          12962\n",
       "Tuen Mun Hospital                            12470\n",
       "Prince Of Wales Hospital                     11361\n",
       "Ruttonjee Hospital                            6419\n",
       "Alice Ho Miu Ling Nethersole Hospital         4768\n",
       "North District Hospital                       3885\n",
       "Tseung Kwan O Hospital                        3020\n",
       "Caritas Medical Centre                        2403\n",
       "Kwong Wah Hospital                            2069\n",
       "Yan Chai Hospital                             1794\n",
       "North Lantau Hospital                          210\n",
       "Pok Oi Hospital                                210\n",
       "Pending                                        192\n",
       "Not Applicable                                  55\n",
       "Name: Name of hospital admitted, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hospitals = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Name of hospital admitted'] = csv['Name of hospital admitted'].astype(np.str).map(lambda ele: ele.title())\n",
    "    if all_hospitals is None:\n",
    "        all_hospitals = csv['Name of hospital admitted']\n",
    "    else:\n",
    "        all_hospitals = all_hospitals.append(csv['Name of hospital admitted'], ignore_index=True)\n",
    "\n",
    "all_hospitals.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospitals_map(hospital):\n",
    "    if (hospital == 'Pending' or hospital == 'Not Applicable' or \n",
    "        hospital == 'Nan'):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return hospital\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Name of hospital admitted'] = csv['Name of hospital admitted'].map(hospitals_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Princess Margaret Hospital                  16194\n",
       "Queen Elizabeth Hospital                    13658\n",
       "Pamela Youde Nethersole Eastern Hospital    13639\n",
       "United Christian Hospital                   13611\n",
       "Queen Mary Hospital                         12962\n",
       "Tuen Mun Hospital                           12470\n",
       "Prince Of Wales Hospital                    11361\n",
       "Ruttonjee Hospital                           6419\n",
       "Alice Ho Miu Ling Nethersole Hospital        4768\n",
       "North District Hospital                      3885\n",
       "Tseung Kwan O Hospital                       3020\n",
       "Caritas Medical Centre                       2403\n",
       "Kwong Wah Hospital                           2069\n",
       "Yan Chai Hospital                            1794\n",
       "North Lantau Hospital                         210\n",
       "Pok Oi Hospital                               210\n",
       "Name: Name of hospital admitted, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hospitals = None\n",
    "\n",
    "for csv in csvs:\n",
    "    if all_hospitals is None:\n",
    "        all_hospitals = csv['Name of hospital admitted']\n",
    "    else:\n",
    "        all_hospitals = all_hospitals.append(csv['Name of hospital admitted'], ignore_index=True)\n",
    "\n",
    "all_hospitals.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospitalised/Discharged/Deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discharged                                   222697\n",
       "Hospitalised                                  55134\n",
       "Deceased                                       2830\n",
       "To Be Provided                                 2703\n",
       "Pending Admission                              1811\n",
       "No Admission                                    946\n",
       "Nan                                             181\n",
       "Discharged (Readmitted On 21/4)                  31\n",
       "Discharged (Readmitted On 18/4)                  11\n",
       "Discharged (Readmitted On 24/4)                  10\n",
       "No Admission As Departed                          9\n",
       "Discharged (Readmitted On 20/4)                   5\n",
       "Discharged (Readmitted In Mainland China)         1\n",
       "Pending                                           1\n",
       "Name: Hospitalised/Discharged/Deceased, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_status = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Hospitalised/Discharged/Deceased'] = csv['Hospitalised/Discharged/Deceased'].astype(np.str).map(lambda ele: ele.title())\n",
    "    if all_status is None:\n",
    "        all_status = csv['Hospitalised/Discharged/Deceased']\n",
    "    else:\n",
    "        all_status = all_status.append(csv['Hospitalised/Discharged/Deceased'], ignore_index=True)\n",
    "\n",
    "all_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_map(status):\n",
    "    if status.startswith('Discharged'):\n",
    "        return 'Discharged'\n",
    "    elif (status == 'Pending Admission' or status == 'To Be Provided' or\n",
    "          status == 'Nan' or status == 'Pending'):\n",
    "        return np.nan\n",
    "    elif (status.startswith('No')):\n",
    "        return 'No Admission'\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Hospitalised/Discharged/Deceased'] = csv['Hospitalised/Discharged/Deceased'].map(status_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discharged      222755\n",
       "Hospitalised     55134\n",
       "Deceased          2830\n",
       "No Admission       955\n",
       "Name: Hospitalised/Discharged/Deceased, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_status = None\n",
    "\n",
    "for csv in csvs:\n",
    "    if all_status is None:\n",
    "        all_status = csv['Hospitalised/Discharged/Deceased']\n",
    "    else:\n",
    "        all_status = all_status.append(csv['Hospitalised/Discharged/Deceased'], ignore_index=True)\n",
    "\n",
    "all_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HK/Non-HK resident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hk Resident        279286\n",
       "Unknown              4047\n",
       "Non-Hk Resident      3036\n",
       "Pending                 1\n",
       "Name: HK/Non-HK resident, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_residence = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['HK/Non-HK resident'] = csv['HK/Non-HK resident'].astype(np.str).map(lambda ele: ele.title())\n",
    "    if all_residence is None:\n",
    "        all_residence = csv['HK/Non-HK resident']\n",
    "    else:\n",
    "        all_residence = all_residence.append(csv['HK/Non-HK resident'], ignore_index=True)\n",
    "\n",
    "all_residence.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residence_map(resident):\n",
    "    if resident == 'Unknown' or resident == 'Pending':\n",
    "        return np.nan\n",
    "    elif resident == 'Hk Resident':\n",
    "        return True\n",
    "    elif resident == 'Non-Hk Resident':\n",
    "        return False\n",
    "    else:\n",
    "        return resident\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['HK/Non-HK resident'] = csv['HK/Non-HK resident'].map(residence_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     279286\n",
       "False      3036\n",
       "Name: HK/Non-HK resident, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_residence = None\n",
    "\n",
    "for csv in csvs:\n",
    "    if all_residence is None:\n",
    "        all_residence = csv['HK/Non-HK resident']\n",
    "    else:\n",
    "        all_residence = all_residence.append(csv['HK/Non-HK resident'], ignore_index=True)\n",
    "\n",
    "all_residence.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imported Case                                        100856\n",
       "Epidemiologically Linked With Local Case              83958\n",
       "Local Case                                            51735\n",
       "Imported                                              16944\n",
       "Possibly Local Case                                   12154\n",
       "Epidemiologically Linked With Possibly Local Case      7502\n",
       "Close Contact Of Local Case                            4630\n",
       "Epidemiologically Linked With Imported Case            3390\n",
       "Possibly Local                                         3080\n",
       "Close Contact Of Possibly Local Case                   1342\n",
       "Close Contact Of Imported Case                          613\n",
       "Epidemiologically Linked With Local Case)               165\n",
       "Pending                                                   1\n",
       "Name: Case classification*, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classification = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Case classification*'] = csv['Case classification*'].astype(np.str).map(lambda ele: ele.title())\n",
    "    if all_classification is None:\n",
    "        all_classification = csv['Case classification*']\n",
    "    else:\n",
    "        all_classification = all_classification.append(csv['Case classification*'], ignore_index=True)\n",
    "\n",
    "all_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_map(clf):\n",
    "    if clf.startswith('Imported'):\n",
    "        return 'Imported'\n",
    "    elif clf == 'Local Case' or clf == 'Possibly Local Case' or clf == 'Possibly Local':\n",
    "        return 'Local'\n",
    "    elif ('Epidemiologically' in clf) and ('Local' in clf):\n",
    "        return 'EL-L'\n",
    "    elif ('Epidemiologically' in clf) and ('Imported' in clf):\n",
    "        return 'EL-I'\n",
    "    elif ('Contact' in clf) and ('Local' in clf):\n",
    "        return 'CC-L'\n",
    "    elif ('Contact' in clf) and ('Import' in clf):\n",
    "        return 'CC-I'\n",
    "    elif clf == 'Pending' or clf == 'Nan':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return clf\n",
    "    \n",
    "for csv in csvs:\n",
    "    csv['Case classification*'] = csv['Case classification*'].map(classification_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imported    117800\n",
       "EL-L         91625\n",
       "Local        66969\n",
       "CC-L          5972\n",
       "EL-I          3390\n",
       "CC-I           613\n",
       "Name: Case classification*, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classification = None\n",
    "\n",
    "for csv in csvs:\n",
    "    if all_classification is None:\n",
    "        all_classification = csv['Case classification*']\n",
    "    else:\n",
    "        all_classification = all_classification.append(csv['Case classification*'], ignore_index=True)\n",
    "\n",
    "all_classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmed/probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confirmed    286222\n",
       "Probable        148\n",
       "Name: Confirmed/probable, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_confirmed = None\n",
    "\n",
    "for csv in csvs:\n",
    "    csv['Confirmed/probable'] = csv['Confirmed/probable'].astype(np.str).map(lambda ele: ele.title())\n",
    "    if all_confirmed is None:\n",
    "        all_confirmed = csv['Confirmed/probable']\n",
    "    else:\n",
    "        all_confirmed = all_confirmed.append(csv['Confirmed/probable'], ignore_index=True)\n",
    "\n",
    "all_confirmed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csvs:\n",
    "    csv['Confirmed/probable'] = csv['Confirmed/probable'].map(lambda conf: conf == 'Confirmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csvs:\n",
    "    csv.rename(columns = {\n",
    "        'Report date': 'Reported',\n",
    "        'Date of onset': 'Onset',\n",
    "        'Name of hospital admitted': 'Admitted Hospital',\n",
    "        'Hospitalised/Discharged/Deceased': 'Status',\n",
    "        'HK/Non-HK resident': 'Resident',\n",
    "        'Case classification*': 'Case Type',\n",
    "        'Confirmed/probable': 'Confirmed',\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Cleaned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2211010d0b3f4d97bd4952a1738aaf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=73, description='df', max=147), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def preview_df(df):\n",
    "    df = max(df, 0)\n",
    "    df = min(df, len(csvs)-1)\n",
    "    return csvs[df].iloc[np.random.randint(len(csvs[df]), size=20)]\n",
    "\n",
    "widgets.interact(preview_df, df=(0, len(csvs)-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Tables into a Progress DF\n",
    "\n",
    "With rows being case ID, and columns being the different statuses. The value will be dates the status first appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data from 03/09/2020\n",
      "Adding data from 02/09/2020\n",
      "Adding data from 01/09/2020\n",
      "Adding data from 31/08/2020\n",
      "Adding data from 30/08/2020\n",
      "Adding data from 29/08/2020\n",
      "Adding data from 28/08/2020\n",
      "Adding data from 27/08/2020\n",
      "Adding data from 26/08/2020\n",
      "Adding data from 25/08/2020\n",
      "Adding data from 24/08/2020\n",
      "Adding data from 23/08/2020\n",
      "Adding data from 22/08/2020\n",
      "Adding data from 21/08/2020\n",
      "Adding data from 20/08/2020\n",
      "Adding data from 19/08/2020\n",
      "Adding data from 18/08/2020\n",
      "Adding data from 17/08/2020\n",
      "Adding data from 16/08/2020\n",
      "Adding data from 15/08/2020\n",
      "Adding data from 14/08/2020\n",
      "Adding data from 13/08/2020\n",
      "Adding data from 12/08/2020\n",
      "Adding data from 11/08/2020\n",
      "Adding data from 10/08/2020\n",
      "Adding data from 09/08/2020\n",
      "Adding data from 07/08/2020\n",
      "Adding data from 06/08/2020\n",
      "Adding data from 05/08/2020\n",
      "Adding data from 03/08/2020\n",
      "Adding data from 02/08/2020\n",
      "Adding data from 01/08/2020\n",
      "Adding data from 31/07/2020\n",
      "Adding data from 30/07/2020\n",
      "Adding data from 29/07/2020\n",
      "Adding data from 28/07/2020\n",
      "Adding data from 27/07/2020\n",
      "Adding data from 26/07/2020\n",
      "Adding data from 25/07/2020\n",
      "Adding data from 24/07/2020\n",
      "Adding data from 23/07/2020\n",
      "Adding data from 22/07/2020\n",
      "Adding data from 21/07/2020\n",
      "Adding data from 20/07/2020\n",
      "Adding data from 19/07/2020\n",
      "Adding data from 18/07/2020\n",
      "Adding data from 17/07/2020\n",
      "Adding data from 16/07/2020\n",
      "Adding data from 15/07/2020\n",
      "Adding data from 14/07/2020\n",
      "Adding data from 13/07/2020\n",
      "Adding data from 12/07/2020\n",
      "Adding data from 11/07/2020\n",
      "Adding data from 10/07/2020\n",
      "Adding data from 09/07/2020\n",
      "Adding data from 08/07/2020\n",
      "Adding data from 07/07/2020\n",
      "Adding data from 06/07/2020\n",
      "Adding data from 05/07/2020\n",
      "Adding data from 04/07/2020\n",
      "Adding data from 03/07/2020\n",
      "Adding data from 02/07/2020\n",
      "Adding data from 01/07/2020\n",
      "Adding data from 30/06/2020\n",
      "Adding data from 29/06/2020\n",
      "Adding data from 28/06/2020\n",
      "Adding data from 27/06/2020\n",
      "Adding data from 26/06/2020\n",
      "Adding data from 25/06/2020\n",
      "Adding data from 24/06/2020\n",
      "Adding data from 23/06/2020\n",
      "Adding data from 22/06/2020\n",
      "Adding data from 21/06/2020\n",
      "Adding data from 20/06/2020\n",
      "Adding data from 19/06/2020\n",
      "Adding data from 18/06/2020\n",
      "Adding data from 17/06/2020\n",
      "Adding data from 16/06/2020\n",
      "Adding data from 15/06/2020\n",
      "Adding data from 14/06/2020\n",
      "Adding data from 13/06/2020\n",
      "Adding data from 12/06/2020\n",
      "Adding data from 11/06/2020\n",
      "Adding data from 10/06/2020\n",
      "Adding data from 09/06/2020\n",
      "Adding data from 08/06/2020\n",
      "Adding data from 07/06/2020\n",
      "Adding data from 06/06/2020\n",
      "Adding data from 05/06/2020\n",
      "Adding data from 04/06/2020\n",
      "Adding data from 03/06/2020\n",
      "Adding data from 02/06/2020\n",
      "Adding data from 01/06/2020\n",
      "Adding data from 31/05/2020\n",
      "Adding data from 30/05/2020\n",
      "Adding data from 29/05/2020\n",
      "Adding data from 28/05/2020\n",
      "Adding data from 27/05/2020\n",
      "Adding data from 25/05/2020\n",
      "Adding data from 23/05/2020\n",
      "Adding data from 22/05/2020\n",
      "Adding data from 21/05/2020\n",
      "Adding data from 19/05/2020\n",
      "Adding data from 18/05/2020\n",
      "Adding data from 17/05/2020\n",
      "Adding data from 16/05/2020\n",
      "Adding data from 15/05/2020\n",
      "Adding data from 14/05/2020\n",
      "Adding data from 13/05/2020\n",
      "Adding data from 12/05/2020\n",
      "Adding data from 11/05/2020\n",
      "Adding data from 10/05/2020\n",
      "Adding data from 09/05/2020\n",
      "Adding data from 08/05/2020\n",
      "Adding data from 07/05/2020\n",
      "Adding data from 06/05/2020\n",
      "Adding data from 05/05/2020\n",
      "Adding data from 04/05/2020\n",
      "Adding data from 03/05/2020\n",
      "Adding data from 02/05/2020\n",
      "Adding data from 01/05/2020\n",
      "Adding data from 30/04/2020\n",
      "Adding data from 29/04/2020\n",
      "Adding data from 28/04/2020\n",
      "Adding data from 27/04/2020\n",
      "Adding data from 26/04/2020\n",
      "Adding data from 25/04/2020\n",
      "Adding data from 24/04/2020\n",
      "Adding data from 23/04/2020\n",
      "Adding data from 22/04/2020\n",
      "Adding data from 21/04/2020\n",
      "Adding data from 20/04/2020\n",
      "Adding data from 19/04/2020\n",
      "Adding data from 18/04/2020\n",
      "Adding data from 17/04/2020\n",
      "Adding data from 16/04/2020\n",
      "Adding data from 15/04/2020\n",
      "Adding data from 14/04/2020\n",
      "Adding data from 13/04/2020\n",
      "Adding data from 12/04/2020\n",
      "Adding data from 11/04/2020\n",
      "Adding data from 10/04/2020\n",
      "Adding data from 09/04/2020\n",
      "Adding data from 08/04/2020\n",
      "Adding data from 07/04/2020\n",
      "Adding data from 06/04/2020\n",
      "Adding data from 05/04/2020\n",
      "Adding data from 04/04/2020\n"
     ]
    }
   ],
   "source": [
    "progress = pd.DataFrame(columns=['Gender', 'Age', 'Resident', 'Case Type', \n",
    "                                 'Reported', 'Onset', 'Asymptomatic', \n",
    "                                 'Hospitalised', 'Discharged', 'Deceased', \n",
    "                                 'Confirmed'])\n",
    "progress.index.name = 'Case no.'\n",
    "\n",
    "def col_map(row, match, day, month):\n",
    "    if row == match:\n",
    "        return pd.to_datetime(day + '/' + month + '/2020', dayfirst=True)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Start from most recent so earlier dates will overwrite later dates\n",
    "for i in range(len(csvs)-1, -1, -1):\n",
    "    csv = csvs[i]\n",
    "    print('Adding data from {}/{}/2020'.format(csv.day, csv.month))\n",
    "    progress = pd.merge(progress, \n",
    "                        csv['Case no.'], \n",
    "                        on=['Case no.'], \n",
    "                        how='outer')\n",
    "    progress.update(csv['Gender'])\n",
    "    progress.update(csv['Age'])\n",
    "    progress.update(csv['Resident'])\n",
    "    progress.update(csv['Case Type'])\n",
    "    progress.update(csv['Asymptomatic'])\n",
    "    progress.update(csv['Reported'])\n",
    "    progress.update(csv['Onset'])\n",
    "    hospitalised = csv['Status'].map(lambda row: col_map(row, 'Hospitalised',\n",
    "                                                         csv.day, csv.month))\n",
    "    hospitalised.name = 'Hospitalised'\n",
    "    progress.update(hospitalised)\n",
    "    discharged = csv['Status'].map(lambda row: col_map(row, 'Discharged',\n",
    "                                                         csv.day, csv.month))\n",
    "    discharged.name = 'Discharged'\n",
    "    progress.update(discharged)\n",
    "    deceased = csv['Status'].map(lambda row: col_map(row, 'Deceased',\n",
    "                                                         csv.day, csv.month))\n",
    "    deceased.name = 'Deceased'\n",
    "    progress.update(deceased)\n",
    "    confirmed = csv['Confirmed'].map(lambda row: col_map(row, True,\n",
    "                                                         csv.day, csv.month))\n",
    "    confirmed.name = 'Confirmed'\n",
    "    progress.update(confirmed)\n",
    "    \n",
    "for date_col in ['Reported', 'Onset', 'Hospitalised', 'Discharged', \n",
    "                 'Deceased', 'Confirmed']:\n",
    "    progress[date_col] = progress[date_col].astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress.set_index(progress['Case no.'], drop=True, inplace=True)\n",
    "progress.drop(columns=['Case no.'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1991125207df4889b83dc08aaf83d5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2415, description='row', max=4830), IntSlider(value=20, description='row…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preview_progress(row, rows=20):\n",
    "    row = max(0, row)\n",
    "    row = min(len(progress)-rows, row)\n",
    "    return progress.iloc[row:row+rows]\n",
    "\n",
    "widgets.interact(preview_progress, row=(0, len(progress) - 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress.to_csv('data\\\\cleaned_progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "output_auto_scroll": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
